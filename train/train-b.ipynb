{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/root/.cache/huggingface/datasets/Logic123456789___json/Logic123456789--Luotuo-QA-B-6abac8029306cc7b/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0804a2759c5a4173bb912bf5f4cefba3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['story', 'questions', 'answers', 'language'],\n",
       "        num_rows: 31464\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"Logic123456789/Luotuo-QA-B\")\n",
    "dataset = raw_datasets\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/Logic123456789___json/Logic123456789--Luotuo-QA-B-6abac8029306cc7b/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-8b8b992eafd9566b.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['story', 'questions', 'answers', 'language'],\n",
       "        num_rows: 25836\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter language is \"Chinese\"\n",
    "dataset = dataset.filter(lambda example: example[\"language\"] == \"Chinese\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at /root/.cache/huggingface/datasets/Logic123456789___json/Logic123456789--Luotuo-QA-B-6abac8029306cc7b/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-af775d9e504fc7fe.arrow and /root/.cache/huggingface/datasets/Logic123456789___json/Logic123456789--Luotuo-QA-B-6abac8029306cc7b/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-6802697e96f2214b.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['story', 'questions', 'answers', 'language'],\n",
       "        num_rows: 23252\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['story', 'questions', 'answers', 'language'],\n",
       "        num_rows: 2584\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset[\"train\"].train_test_split(train_size=0.9, seed=42)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6ff5aa4037a4d45992148a42f51e5c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)enization_chatglm.py:   0%|          | 0.00/17.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "base_model = \"THUDM/chatglm-6b\"\n",
    "base_model_revision = \"969290547e761b20fdb96b0602b4fd8d863bbb85\"\n",
    "base_model_revision = None\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model, \n",
    "    revision=base_model_revision, \n",
    "    trust_remote_code=True,\n",
    "    # padding_side=\"right\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "def preprocess(tokenizer: PreTrainedTokenizer, config, context, target, max_seq_length):\n",
    "    context_tokens = tokenizer.encode(\n",
    "        context,\n",
    "        max_length=max_seq_length,\n",
    "        truncation=True,\n",
    "    )\n",
    "    target_tokens = tokenizer.encode(\n",
    "        target,\n",
    "        max_length=max_seq_length,\n",
    "        truncation=True,\n",
    "        add_special_tokens=False,\n",
    "    )\n",
    "    return dict(\n",
    "        input_ids=context_tokens + target_tokens + [config.eos_token_id],\n",
    "        seq_len=len(context_tokens),\n",
    "    )\n",
    "\n",
    "\n",
    "def get_context_item(story: str, q1, answer):\n",
    "    if q1 == \"\" or answer == \"\":\n",
    "        return None, None, None\n",
    "    origin_question = q1\n",
    "    context = f\"\"\"给你下面的文本和问题，请先给出一个对应问题的同义转述，再给出问题的答案。\n",
    "文本为：{story}\n",
    "问题为：{origin_question}\n",
    "\"\"\"\n",
    "    target = f\"\"\"答案为：{answer}\"\"\"\n",
    "    return context, origin_question, target\n",
    "\n",
    "max_seq_length = 1024\n",
    "skip_overlength = False\n",
    "device_map = \"auto\"\n",
    "config = transformers.AutoConfig.from_pretrained(base_model, trust_remote_code=True, device_map=device_map)\n",
    "def tokenize_and_split(examples):\n",
    "    result = {\n",
    "        \"input_ids\": [],\n",
    "        \"seq_len\": [],\n",
    "    }\n",
    "    for i, story in enumerate(examples[\"story\"]):\n",
    "        for j, (question, answer) in enumerate(zip(examples[\"questions\"][i], examples[\"answers\"][i])):\n",
    "            # print(i, story, question, answer)\n",
    "            context, origin_question, target = get_context_item(story, question, answer)\n",
    "            if context is not None:\n",
    "                feature = preprocess(tokenizer, config, context, target, max_seq_length)\n",
    "                if skip_overlength and len(feature[\"input_ids\"]) > max_seq_length:\n",
    "                    continue\n",
    "                feature[\"input_ids\"] = feature[\"input_ids\"][:max_seq_length]\n",
    "                result[\"input_ids\"].append(feature[\"input_ids\"])\n",
    "                result[\"seq_len\"].append(feature[\"seq_len\"])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4f34e9d75ca4b8089d45d8486dbd990",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=16):   0%|          | 0/23252 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f5446fef4b04eec97f2bac09bf6e8f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=16):   0%|          | 0/2584 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'seq_len'],\n",
       "        num_rows: 116260\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'seq_len'],\n",
       "        num_rows: 12920\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(tokenize_and_split, batched=True, num_proc=16, remove_columns=dataset['train'].column_names)\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [5,\n",
       "  65696,\n",
       "  72751,\n",
       "  71455,\n",
       "  63826,\n",
       "  63963,\n",
       "  6,\n",
       "  106748,\n",
       "  71432,\n",
       "  63866,\n",
       "  68823,\n",
       "  71167,\n",
       "  64095,\n",
       "  64899,\n",
       "  64191,\n",
       "  69568,\n",
       "  6,\n",
       "  63921,\n",
       "  71432,\n",
       "  122021,\n",
       "  63823,\n",
       "  4,\n",
       "  71455,\n",
       "  63834,\n",
       "  12,\n",
       "  22,\n",
       "  63827,\n",
       "  69311,\n",
       "  78955,\n",
       "  72576,\n",
       "  74387,\n",
       "  75604,\n",
       "  67215,\n",
       "  867,\n",
       "  63992,\n",
       "  64616,\n",
       "  76549,\n",
       "  116774,\n",
       "  63826,\n",
       "  82723,\n",
       "  66815,\n",
       "  76549,\n",
       "  80018,\n",
       "  6,\n",
       "  69374,\n",
       "  64829,\n",
       "  64276,\n",
       "  68304,\n",
       "  63825,\n",
       "  867,\n",
       "  63992,\n",
       "  70974,\n",
       "  7,\n",
       "  63827,\n",
       "  64602,\n",
       "  65278,\n",
       "  63839,\n",
       "  68231,\n",
       "  64276,\n",
       "  84352,\n",
       "  65278,\n",
       "  68427,\n",
       "  71131,\n",
       "  64536,\n",
       "  6,\n",
       "  71432,\n",
       "  68659,\n",
       "  65646,\n",
       "  63833,\n",
       "  17,\n",
       "  65520,\n",
       "  64166,\n",
       "  17,\n",
       "  66076,\n",
       "  93964,\n",
       "  103725,\n",
       "  64638,\n",
       "  65278,\n",
       "  64536,\n",
       "  76968,\n",
       "  68486,\n",
       "  65461,\n",
       "  6,\n",
       "  64344,\n",
       "  75511,\n",
       "  66156,\n",
       "  76549,\n",
       "  70424,\n",
       "  65690,\n",
       "  66815,\n",
       "  76549,\n",
       "  66905,\n",
       "  6,\n",
       "  78930,\n",
       "  66459,\n",
       "  78368,\n",
       "  66385,\n",
       "  114895,\n",
       "  67110,\n",
       "  64398,\n",
       "  64285,\n",
       "  7,\n",
       "  65951,\n",
       "  67124,\n",
       "  6,\n",
       "  64030,\n",
       "  64113,\n",
       "  108580,\n",
       "  67110,\n",
       "  72576,\n",
       "  74387,\n",
       "  63833,\n",
       "  100630,\n",
       "  64179,\n",
       "  7,\n",
       "  22,\n",
       "  4,\n",
       "  63963,\n",
       "  63834,\n",
       "  12,\n",
       "  64652,\n",
       "  67215,\n",
       "  867,\n",
       "  63992,\n",
       "  63827,\n",
       "  72576,\n",
       "  74387,\n",
       "  63833,\n",
       "  64616,\n",
       "  66758,\n",
       "  31,\n",
       "  4,\n",
       "  130001,\n",
       "  130004,\n",
       "  75466,\n",
       "  63834,\n",
       "  12,\n",
       "  63917,\n",
       "  64616,\n",
       "  76549,\n",
       "  116774,\n",
       "  63826,\n",
       "  82723,\n",
       "  66815,\n",
       "  76549,\n",
       "  64471,\n",
       "  63823,\n",
       "  130005],\n",
       " 'seq_len': 134}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset[\"train\"][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda-11.7/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /usr/local/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import peft\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "from transformers import AutoModel, AutoConfig\n",
    "import torch.nn as nn\n",
    "\n",
    "class CastOutputToFloat(nn.Sequential):\n",
    "    def forward(self, x):\n",
    "        return super().forward(x).to(torch.float32)\n",
    "\n",
    "def load_train_model(model_path, lora_rank, model_revision: str = None, cache_dir: str = None, device_map: str=None, ddp: bool = False, load_in_8bit: bool = False):\n",
    "    # init model\n",
    "    model = AutoModel.from_pretrained(\n",
    "        model_path, trust_remote_code=True, device_map=device_map, revision=model_revision, cache_dir = cache_dir\n",
    "    )\n",
    "    model.gradient_checkpointing_enable()\n",
    "    model.enable_input_require_grads()\n",
    "    model.is_parallelizable = True\n",
    "    model.model_parallel = True\n",
    "    model.lm_head = CastOutputToFloat(model.lm_head)\n",
    "    model.config.use_cache = (\n",
    "        False  # silence the warnings. Please re-enable for inference!\n",
    "    )\n",
    "    # setup peft\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        inference_mode=False,\n",
    "        r=lora_rank,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.1,\n",
    "    )\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_collator(tokenizer, features: list, to_device = None) -> dict:\n",
    "    len_ids = [len(feature[\"input_ids\"]) for feature in features]\n",
    "    longest = max(len_ids)\n",
    "    input_ids = []\n",
    "    labels_list = []\n",
    "    for ids_l, feature in sorted(zip(len_ids, features), key=lambda x: -x[0]):\n",
    "        ids = feature[\"input_ids\"]\n",
    "        seq_len = feature[\"seq_len\"]\n",
    "        labels = (\n",
    "            [-100] * (seq_len - 1) + ids[(seq_len - 1) :] + [-100] * (longest - ids_l)\n",
    "        )\n",
    "        ids = ids + [tokenizer.pad_token_id] * (longest - ids_l)\n",
    "        _ids = torch.LongTensor(ids)\n",
    "        labels_list.append(torch.LongTensor(labels))\n",
    "        input_ids.append(_ids)\n",
    "    input_ids = torch.stack(input_ids)\n",
    "    labels = torch.stack(labels_list)\n",
    "    if to_device:\n",
    "        input_ids = input_ids.to(to_device)\n",
    "        labels = labels.to(to_device)\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"labels\": labels,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62fab6e95fb54ebeafd9cde7616e33b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/modeling_chatglm.py:   0%|          | 0.00/57.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "967ff02452b7401593781c509b0c41eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25935b0c685b4139b0d191f7c356a88a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00001-of-00008.bin:   0%|          | 0.00/1.74G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fba55209add14f1ca3a026f6d1ad9c0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00002-of-00008.bin:   0%|          | 0.00/1.88G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5746256b6e0e4d1789335987bc70591f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00003-of-00008.bin:   0%|          | 0.00/1.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f85f5b0db734bfeae0cec65654479dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00004-of-00008.bin:   0%|          | 0.00/1.91G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c606e6a713a4b3d83ed4ab683895a54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00005-of-00008.bin:   0%|          | 0.00/1.88G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cd4ae2f053142a7ac36b6e1d72d41b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00006-of-00008.bin:   0%|          | 0.00/1.88G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "006c18432fb842808c1d559bf9b20153",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00007-of-00008.bin:   0%|          | 0.00/1.07G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1729138198d4464d9428bfdd1a768e60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00008-of-00008.bin:   0%|          | 0.00/1.07G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3199e9342a404e73bdebd33838654ae2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import Trainer, TrainingArguments, IntervalStrategy\n",
    "from os import path\n",
    "\n",
    "per_device_train_batch_size=32\n",
    "num_train_epochs = 8\n",
    "max_steps = -1\n",
    "project_name = \"luotuo-qa-b\"\n",
    "\n",
    "output_path = \"output/\"+project_name+\"-\"+str(\"v1\")\n",
    "\n",
    "def get_steps_in_epoch(pos: float):\n",
    "    res = int(pos * (dataset[\"train\"].num_rows // per_device_train_batch_size))\n",
    "    return res if res > 0 else 1\n",
    "\n",
    "model = load_train_model(base_model, 8, model_revision=base_model_revision, device_map=device_map, load_in_8bit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mliasece\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspaces/src/ai/luotuo-qa-tuning/train/wandb/run-20230525_050839-rlul813q</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/liasece/huggingface/runs/rlul813q' target=\"_blank\">earthy-butterfly-63</a></strong> to <a href='https://wandb.ai/liasece/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/liasece/huggingface' target=\"_blank\">https://wandb.ai/liasece/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/liasece/huggingface/runs/rlul813q' target=\"_blank\">https://wandb.ai/liasece/huggingface/runs/rlul813q</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TorchRuntimeError",
     "evalue": "\n\nfrom user code:\n   File \"/root/.cache/huggingface/modules/transformers_modules/THUDM/chatglm-6b/1d240ba371910e9282298d4592532d7f0f3e9f3e/modeling_chatglm.py\", line 926, in forward\n    inputs_embeds = self.word_embeddings(input_ids)\n\nSet torch._dynamo.config.verbose=True for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    torch._dynamo.config.suppress_errors = True\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_dynamo/utils.py:1199\u001b[0m, in \u001b[0;36mrun_node\u001b[0;34m(output_graph, node, args, kwargs, nnmodule)\u001b[0m\n\u001b[1;32m   1198\u001b[0m     \u001b[39massert\u001b[39;00m nnmodule \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1199\u001b[0m     \u001b[39mreturn\u001b[39;00m nnmodule(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1200\u001b[0m \u001b[39melif\u001b[39;00m op \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mget_attr\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1538\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m     args \u001b[39m=\u001b[39m bw_hook\u001b[39m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1538\u001b[0m result \u001b[39m=\u001b[39m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1539\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 162\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[1;32m    163\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[1;32m    164\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2210\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/utils/_stats.py:20\u001b[0m, in \u001b[0;36mcount.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     19\u001b[0m simple_call_counter[fn\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m] \u001b[39m=\u001b[39m simple_call_counter[fn\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m] \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m---> 20\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_subclasses/fake_tensor.py:987\u001b[0m, in \u001b[0;36mFakeTensorMode.__torch_dispatch__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    986\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 987\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch(func, types, args, kwargs)\n\u001b[1;32m    988\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_subclasses/fake_tensor.py:1066\u001b[0m, in \u001b[0;36mFakeTensorMode.dispatch\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1064\u001b[0m     \u001b[39mreturn\u001b[39;00m converter(\u001b[39mself\u001b[39m, args[\u001b[39m0\u001b[39m])\n\u001b[0;32m-> 1066\u001b[0m args, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvalidate_and_convert_non_fake_tensors(\n\u001b[1;32m   1067\u001b[0m     func, converter, args, kwargs\n\u001b[1;32m   1068\u001b[0m )\n\u001b[1;32m   1070\u001b[0m \u001b[39m# The current constant handling only support tracing systems\u001b[39;00m\n\u001b[1;32m   1071\u001b[0m \u001b[39m# (aot autograd, torchdynamo) where each operation is run consecutively.\u001b[39;00m\n\u001b[1;32m   1072\u001b[0m \u001b[39m# Because each operation is run in order, we can trace out and support\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1078\u001b[0m \n\u001b[1;32m   1079\u001b[0m \u001b[39m# We dispatch size/stride/numel on the FakeTensor not its constant, so bail on inplace_view\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_subclasses/fake_tensor.py:1220\u001b[0m, in \u001b[0;36mFakeTensorMode.validate_and_convert_non_fake_tensors\u001b[0;34m(self, func, converter, args, kwargs)\u001b[0m\n\u001b[1;32m   1218\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n\u001b[0;32m-> 1220\u001b[0m \u001b[39mreturn\u001b[39;00m tree_map_only(\n\u001b[1;32m   1221\u001b[0m     torch\u001b[39m.\u001b[39;49mTensor,\n\u001b[1;32m   1222\u001b[0m     validate,\n\u001b[1;32m   1223\u001b[0m     (args, kwargs),\n\u001b[1;32m   1224\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/utils/_pytree.py:266\u001b[0m, in \u001b[0;36mtree_map_only\u001b[0;34m(ty, fn, pytree)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtree_map_only\u001b[39m(ty: TypeAny, fn: FnAny[Any], pytree: PyTree) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m PyTree:\n\u001b[0;32m--> 266\u001b[0m     \u001b[39mreturn\u001b[39;00m tree_map(map_only(ty)(fn), pytree)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/utils/_pytree.py:196\u001b[0m, in \u001b[0;36mtree_map\u001b[0;34m(fn, pytree)\u001b[0m\n\u001b[1;32m    195\u001b[0m flat_args, spec \u001b[39m=\u001b[39m tree_flatten(pytree)\n\u001b[0;32m--> 196\u001b[0m \u001b[39mreturn\u001b[39;00m tree_unflatten([fn(i) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m flat_args], spec)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/utils/_pytree.py:196\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    195\u001b[0m flat_args, spec \u001b[39m=\u001b[39m tree_flatten(pytree)\n\u001b[0;32m--> 196\u001b[0m \u001b[39mreturn\u001b[39;00m tree_unflatten([fn(i) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m flat_args], spec)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/utils/_pytree.py:247\u001b[0m, in \u001b[0;36mmap_only.<locals>.deco.<locals>.inner\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(x, ty):\n\u001b[0;32m--> 247\u001b[0m     \u001b[39mreturn\u001b[39;00m f(x)\n\u001b[1;32m    248\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_subclasses/fake_tensor.py:1212\u001b[0m, in \u001b[0;36mFakeTensorMode.validate_and_convert_non_fake_tensors.<locals>.validate\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1211\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mallow_non_fake_inputs:\n\u001b[0;32m-> 1212\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\n\u001b[1;32m   1213\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPlease convert all Tensors to FakeTensors first or instantiate FakeTensorMode \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1214\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mwith \u001b[39m\u001b[39m'\u001b[39m\u001b[39mallow_non_fake_inputs\u001b[39m\u001b[39m'\u001b[39m\u001b[39m. Found in \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m(*\u001b[39m\u001b[39m{\u001b[39;00margs\u001b[39m}\u001b[39;00m\u001b[39m, **\u001b[39m\u001b[39m{\u001b[39;00mkwargs\u001b[39m}\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1215\u001b[0m     )\n\u001b[1;32m   1217\u001b[0m \u001b[39mreturn\u001b[39;00m converter(\u001b[39mself\u001b[39m, x)\n",
      "\u001b[0;31mException\u001b[0m: Please convert all Tensors to FakeTensors first or instantiate FakeTensorMode with 'allow_non_fake_inputs'. Found in aten.embedding.default(*(Parameter containing:\ntensor([[-8.4076e-03, -9.3689e-03, -5.4436e-03,  ..., -6.4545e-03,\n          1.6998e-02,  1.1108e-02],\n        [-1.0071e-02,  5.8022e-03,  4.8018e-04,  ..., -4.2701e-04,\n          1.0252e-03, -1.6556e-03],\n        [ 1.9424e-02,  6.3477e-03,  2.4933e-02,  ...,  5.7297e-03,\n          1.2512e-02,  9.4147e-03],\n        ...,\n        [-1.0078e-02,  3.0041e-03,  2.4376e-03,  ..., -4.7684e-06,\n          1.5430e-03,  1.1053e-03],\n        [-9.9945e-03,  4.4479e-03,  6.2141e-03,  ...,  1.7560e-04,\n          1.4286e-03, -1.1883e-03],\n        [-9.3536e-03,  1.7376e-03,  5.7373e-03,  ..., -1.0910e-03,\n          4.3945e-03, -1.2541e-03]], device='cuda:0', dtype=torch.float16), FakeTensor(FakeTensor(..., device='meta', size=(32, 496), dtype=torch.int64), cuda:0)), **{}) ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_dynamo/utils.py:1152\u001b[0m, in \u001b[0;36mget_fake_value\u001b[0;34m(node, tx)\u001b[0m\n\u001b[1;32m   1151\u001b[0m     \u001b[39mwith\u001b[39;00m tx\u001b[39m.\u001b[39mfake_mode, enable_python_dispatcher():\n\u001b[0;32m-> 1152\u001b[0m         \u001b[39mreturn\u001b[39;00m wrap_fake_exception(\n\u001b[1;32m   1153\u001b[0m             \u001b[39mlambda\u001b[39;49;00m: run_node(tx\u001b[39m.\u001b[39;49moutput, node, args, kwargs, nnmodule)\n\u001b[1;32m   1154\u001b[0m         )\n\u001b[1;32m   1155\u001b[0m \u001b[39mexcept\u001b[39;00m Unsupported:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_dynamo/utils.py:808\u001b[0m, in \u001b[0;36mwrap_fake_exception\u001b[0;34m(fn)\u001b[0m\n\u001b[1;32m    807\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 808\u001b[0m     \u001b[39mreturn\u001b[39;00m fn()\n\u001b[1;32m    809\u001b[0m \u001b[39mexcept\u001b[39;00m UnsupportedFakeTensorException \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_dynamo/utils.py:1153\u001b[0m, in \u001b[0;36mget_fake_value.<locals>.<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1151\u001b[0m     \u001b[39mwith\u001b[39;00m tx\u001b[39m.\u001b[39mfake_mode, enable_python_dispatcher():\n\u001b[1;32m   1152\u001b[0m         \u001b[39mreturn\u001b[39;00m wrap_fake_exception(\n\u001b[0;32m-> 1153\u001b[0m             \u001b[39mlambda\u001b[39;00m: run_node(tx\u001b[39m.\u001b[39;49moutput, node, args, kwargs, nnmodule)\n\u001b[1;32m   1154\u001b[0m         )\n\u001b[1;32m   1155\u001b[0m \u001b[39mexcept\u001b[39;00m Unsupported:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_dynamo/utils.py:1206\u001b[0m, in \u001b[0;36mrun_node\u001b[0;34m(output_graph, node, args, kwargs, nnmodule)\u001b[0m\n\u001b[1;32m   1205\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m-> 1206\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1207\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFailed running \u001b[39m\u001b[39m{\u001b[39;00mop\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mnode\u001b[39m.\u001b[39mtarget\u001b[39m}\u001b[39;00m\u001b[39m(*\u001b[39m\u001b[39m{\u001b[39;00margs\u001b[39m}\u001b[39;00m\u001b[39m, **\u001b[39m\u001b[39m{\u001b[39;00mkwargs\u001b[39m}\u001b[39;00m\u001b[39m):\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m(scroll up for backtrace)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1208\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m   1209\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(op)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed running call_module self_word_embeddings(*(FakeTensor(FakeTensor(..., device='meta', size=(32, 496), dtype=torch.int64), cuda:0),), **{}):\nPlease convert all Tensors to FakeTensors first or instantiate FakeTensorMode with 'allow_non_fake_inputs'. Found in aten.embedding.default(*(Parameter containing:\ntensor([[-8.4076e-03, -9.3689e-03, -5.4436e-03,  ..., -6.4545e-03,\n          1.6998e-02,  1.1108e-02],\n        [-1.0071e-02,  5.8022e-03,  4.8018e-04,  ..., -4.2701e-04,\n          1.0252e-03, -1.6556e-03],\n        [ 1.9424e-02,  6.3477e-03,  2.4933e-02,  ...,  5.7297e-03,\n          1.2512e-02,  9.4147e-03],\n        ...,\n        [-1.0078e-02,  3.0041e-03,  2.4376e-03,  ..., -4.7684e-06,\n          1.5430e-03,  1.1053e-03],\n        [-9.9945e-03,  4.4479e-03,  6.2141e-03,  ...,  1.7560e-04,\n          1.4286e-03, -1.1883e-03],\n        [-9.3536e-03,  1.7376e-03,  5.7373e-03,  ..., -1.0910e-03,\n          4.3945e-03, -1.2541e-03]], device='cuda:0', dtype=torch.float16), FakeTensor(FakeTensor(..., device='meta', size=(32, 496), dtype=torch.int64), cuda:0)), **{}) \n(scroll up for backtrace)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTorchRuntimeError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 44\u001b[0m\n\u001b[1;32m     42\u001b[0m resume_from_checkpoint \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[39m# resume_from_checkpoint = path.join(output_path, \"checkpoint-6410\")\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain(resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint)\n\u001b[1;32m     45\u001b[0m \u001b[39m# save model\u001b[39;00m\n\u001b[1;32m     46\u001b[0m model\u001b[39m.\u001b[39msave_pretrained(output_path)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/transformers/trainer.py:1633\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1628\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[1;32m   1630\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1631\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1632\u001b[0m )\n\u001b[0;32m-> 1633\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1634\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1635\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1636\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1637\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1638\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/transformers/trainer.py:1902\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1900\u001b[0m         tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1901\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1902\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[1;32m   1904\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1905\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1906\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1907\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1908\u001b[0m ):\n\u001b[1;32m   1909\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1910\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/transformers/trainer.py:2645\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2642\u001b[0m     \u001b[39mreturn\u001b[39;00m loss_mb\u001b[39m.\u001b[39mreduce_mean()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m   2644\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2645\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss(model, inputs)\n\u001b[1;32m   2647\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mn_gpu \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   2648\u001b[0m     loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mmean()  \u001b[39m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m, in \u001b[0;36mModifiedTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_loss\u001b[39m(\u001b[39mself\u001b[39m, model, inputs, return_outputs\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m----> 3\u001b[0m     \u001b[39mreturn\u001b[39;00m model(\n\u001b[1;32m      4\u001b[0m         input_ids\u001b[39m=\u001b[39;49minputs[\u001b[39m\"\u001b[39;49m\u001b[39minput_ids\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m      5\u001b[0m         labels\u001b[39m=\u001b[39;49minputs[\u001b[39m\"\u001b[39;49m\u001b[39mlabels\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m      6\u001b[0m     )\u001b[39m.\u001b[39mloss\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:82\u001b[0m, in \u001b[0;36mOptimizedModule.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> 82\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdynamo_ctx(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_orig_mod\u001b[39m.\u001b[39;49mforward)(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:209\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m dynamic_ctx\u001b[39m.\u001b[39m\u001b[39m__enter__\u001b[39m()\n\u001b[1;32m    208\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 209\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    210\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/peft/peft_model.py:686\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m    684\u001b[0m peft_config \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactive_peft_config\n\u001b[1;32m    685\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(peft_config, PromptLearningConfig):\n\u001b[0;32m--> 686\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_model(\n\u001b[1;32m    687\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m    688\u001b[0m         attention_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[1;32m    689\u001b[0m         inputs_embeds\u001b[39m=\u001b[39minputs_embeds,\n\u001b[1;32m    690\u001b[0m         labels\u001b[39m=\u001b[39mlabels,\n\u001b[1;32m    691\u001b[0m         output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[1;32m    692\u001b[0m         output_hidden_states\u001b[39m=\u001b[39moutput_hidden_states,\n\u001b[1;32m    693\u001b[0m         return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[1;32m    694\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    695\u001b[0m     )\n\u001b[1;32m    697\u001b[0m batch_size \u001b[39m=\u001b[39m input_ids\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m    698\u001b[0m \u001b[39mif\u001b[39;00m attention_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[39m# concat prompt attention mask\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/accelerate/hooks.py:160\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(old_forward)\n\u001b[1;32m    159\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnew_forward\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 160\u001b[0m     args, kwargs \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpre_forward(module, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    161\u001b[0m     \u001b[39mif\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mno_grad:\n\u001b[1;32m    162\u001b[0m         \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36m<graph break in new_forward>\u001b[0;34m(___stack0)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/THUDM/chatglm-6b/1d240ba371910e9282298d4592532d7f0f3e9f3e/modeling_chatglm.py:1190\u001b[0m, in \u001b[0;36mChatGLMForConditionalGeneration.forward\u001b[0;34m(self, input_ids, position_ids, attention_mask, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1187\u001b[0m use_cache \u001b[39m=\u001b[39m use_cache \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_cache\n\u001b[1;32m   1188\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1190\u001b[0m transformer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer(\n\u001b[1;32m   1191\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1192\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[1;32m   1193\u001b[0m     attention_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[1;32m   1194\u001b[0m     past_key_values\u001b[39m=\u001b[39mpast_key_values,\n\u001b[1;32m   1195\u001b[0m     inputs_embeds\u001b[39m=\u001b[39minputs_embeds,\n\u001b[1;32m   1196\u001b[0m     use_cache\u001b[39m=\u001b[39muse_cache,\n\u001b[1;32m   1197\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[1;32m   1198\u001b[0m     output_hidden_states\u001b[39m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   1199\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[1;32m   1200\u001b[0m )\n\u001b[1;32m   1202\u001b[0m hidden_states \u001b[39m=\u001b[39m transformer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1204\u001b[0m lm_logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head(hidden_states)\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mcontiguous()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/accelerate/hooks.py:160\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(old_forward)\n\u001b[1;32m    159\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnew_forward\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 160\u001b[0m     args, kwargs \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpre_forward(module, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    161\u001b[0m     \u001b[39mif\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mno_grad:\n\u001b[1;32m    162\u001b[0m         \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36m<graph break in new_forward>\u001b[0;34m(___stack0)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:337\u001b[0m, in \u001b[0;36mcatch_errors_wrapper.<locals>.catch_errors\u001b[0;34m(frame, cache_size)\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[39mreturn\u001b[39;00m hijacked_callback(frame, cache_size, hooks)\n\u001b[1;32m    336\u001b[0m \u001b[39mwith\u001b[39;00m compile_lock:\n\u001b[0;32m--> 337\u001b[0m     \u001b[39mreturn\u001b[39;00m callback(frame, cache_size, hooks)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:404\u001b[0m, in \u001b[0;36mconvert_frame.<locals>._convert_frame\u001b[0;34m(frame, cache_size, hooks)\u001b[0m\n\u001b[1;32m    402\u001b[0m counters[\u001b[39m\"\u001b[39m\u001b[39mframes\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtotal\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    403\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 404\u001b[0m     result \u001b[39m=\u001b[39m inner_convert(frame, cache_size, hooks)\n\u001b[1;32m    405\u001b[0m     counters[\u001b[39m\"\u001b[39m\u001b[39mframes\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mok\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    406\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:104\u001b[0m, in \u001b[0;36mwrap_convert_context.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m torch\u001b[39m.\u001b[39mfx\u001b[39m.\u001b[39mgraph_module\u001b[39m.\u001b[39m_forward_from_src \u001b[39m=\u001b[39m fx_forward_from_src_skip_result\n\u001b[1;32m    103\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 104\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    105\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    106\u001b[0m     torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_set_grad_enabled(prior_grad_mode)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:262\u001b[0m, in \u001b[0;36mconvert_frame_assert.<locals>._convert_frame_assert\u001b[0;34m(frame, cache_size, hooks)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[39mglobal\u001b[39;00m initial_grad_state\n\u001b[1;32m    260\u001b[0m initial_grad_state \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mis_grad_enabled()\n\u001b[0;32m--> 262\u001b[0m \u001b[39mreturn\u001b[39;00m _compile(\n\u001b[1;32m    263\u001b[0m     frame\u001b[39m.\u001b[39;49mf_code,\n\u001b[1;32m    264\u001b[0m     frame\u001b[39m.\u001b[39;49mf_globals,\n\u001b[1;32m    265\u001b[0m     frame\u001b[39m.\u001b[39;49mf_locals,\n\u001b[1;32m    266\u001b[0m     frame\u001b[39m.\u001b[39;49mf_builtins,\n\u001b[1;32m    267\u001b[0m     compiler_fn,\n\u001b[1;32m    268\u001b[0m     one_graph,\n\u001b[1;32m    269\u001b[0m     export,\n\u001b[1;32m    270\u001b[0m     hooks,\n\u001b[1;32m    271\u001b[0m     frame,\n\u001b[1;32m    272\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_dynamo/utils.py:163\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m     compilation_metrics[key] \u001b[39m=\u001b[39m []\n\u001b[1;32m    162\u001b[0m t0 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m--> 163\u001b[0m r \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    164\u001b[0m time_spent \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m t0\n\u001b[1;32m    165\u001b[0m \u001b[39m# print(f\"Dynamo timer: key={key}, latency={latency:.2f} sec\")\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:324\u001b[0m, in \u001b[0;36m_compile\u001b[0;34m(code, globals, locals, builtins, compiler_fn, one_graph, export, hooks, frame)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[39mfor\u001b[39;00m attempt \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mcount():\n\u001b[1;32m    323\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 324\u001b[0m         out_code \u001b[39m=\u001b[39m transform_code_object(code, transform)\n\u001b[1;32m    325\u001b[0m         orig_code_map[out_code] \u001b[39m=\u001b[39m code\n\u001b[1;32m    326\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py:445\u001b[0m, in \u001b[0;36mtransform_code_object\u001b[0;34m(code, transformations, safe)\u001b[0m\n\u001b[1;32m    442\u001b[0m instructions \u001b[39m=\u001b[39m cleaned_instructions(code, safe)\n\u001b[1;32m    443\u001b[0m propagate_line_nums(instructions)\n\u001b[0;32m--> 445\u001b[0m transformations(instructions, code_options)\n\u001b[1;32m    446\u001b[0m \u001b[39mreturn\u001b[39;00m clean_and_assemble_instructions(instructions, keys, code_options)[\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:311\u001b[0m, in \u001b[0;36m_compile.<locals>.transform\u001b[0;34m(instructions, code_options)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[39mnonlocal\u001b[39;00m output\n\u001b[1;32m    299\u001b[0m tracer \u001b[39m=\u001b[39m InstructionTranslator(\n\u001b[1;32m    300\u001b[0m     instructions,\n\u001b[1;32m    301\u001b[0m     code,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    309\u001b[0m     mutated_closure_cell_contents,\n\u001b[1;32m    310\u001b[0m )\n\u001b[0;32m--> 311\u001b[0m tracer\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m    312\u001b[0m output \u001b[39m=\u001b[39m tracer\u001b[39m.\u001b[39moutput\n\u001b[1;32m    313\u001b[0m \u001b[39massert\u001b[39;00m output \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:1726\u001b[0m, in \u001b[0;36mInstructionTranslator.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1724\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m   1725\u001b[0m     _step_logger()(logging\u001b[39m.\u001b[39mINFO, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtorchdynamo start tracing \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_code\u001b[39m.\u001b[39mco_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1726\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mrun()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:576\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    572\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput\u001b[39m.\u001b[39mpush_tx(\u001b[39mself\u001b[39m)\n\u001b[1;32m    573\u001b[0m     \u001b[39mwhile\u001b[39;00m (\n\u001b[1;32m    574\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minstruction_pointer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    575\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput\u001b[39m.\u001b[39mshould_exit\n\u001b[0;32m--> 576\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m    577\u001b[0m     ):\n\u001b[1;32m    578\u001b[0m         \u001b[39mpass\u001b[39;00m\n\u001b[1;32m    579\u001b[0m \u001b[39mexcept\u001b[39;00m BackendCompilerFailed:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:540\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    538\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, inst\u001b[39m.\u001b[39mopname):\n\u001b[1;32m    539\u001b[0m         unimplemented(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmissing: \u001b[39m\u001b[39m{\u001b[39;00minst\u001b[39m.\u001b[39mopname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 540\u001b[0m     \u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, inst\u001b[39m.\u001b[39;49mopname)(inst)\n\u001b[1;32m    542\u001b[0m     \u001b[39mreturn\u001b[39;00m inst\u001b[39m.\u001b[39mopname \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mRETURN_VALUE\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    543\u001b[0m \u001b[39mexcept\u001b[39;00m BackendCompilerFailed:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:342\u001b[0m, in \u001b[0;36mbreak_graph_if_unsupported.<locals>.decorator.<locals>.wrapper\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m    340\u001b[0m reason \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    341\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 342\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_fn(\u001b[39mself\u001b[39;49m, inst)\n\u001b[1;32m    343\u001b[0m \u001b[39mexcept\u001b[39;00m Unsupported \u001b[39mas\u001b[39;00m excp:\n\u001b[1;32m    344\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhas_backedge() \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshould_compile_partial_graph():\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:965\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.CALL_FUNCTION\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m    963\u001b[0m args \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpopn(inst\u001b[39m.\u001b[39margval)\n\u001b[1;32m    964\u001b[0m fn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpop()\n\u001b[0;32m--> 965\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcall_function(fn, args, {})\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:474\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.call_function\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(kwargs, \u001b[39mdict\u001b[39m)\n\u001b[1;32m    470\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mall\u001b[39m(\n\u001b[1;32m    471\u001b[0m     \u001b[39misinstance\u001b[39m(x, VariableTracker)\n\u001b[1;32m    472\u001b[0m     \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(args, kwargs\u001b[39m.\u001b[39mvalues())\n\u001b[1;32m    473\u001b[0m )\n\u001b[0;32m--> 474\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpush(fn\u001b[39m.\u001b[39;49mcall_function(\u001b[39mself\u001b[39;49m, args, kwargs))\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_dynamo/variables/nn_module.py:203\u001b[0m, in \u001b[0;36mNNModuleVariable.call_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule_type \u001b[39m=\u001b[39m mod\u001b[39m.\u001b[39mcls_to_become\n\u001b[1;32m    201\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mbuilder\u001b[39;00m \u001b[39mimport\u001b[39;00m wrap_fx_proxy\n\u001b[0;32m--> 203\u001b[0m     \u001b[39mreturn\u001b[39;00m wrap_fx_proxy(\n\u001b[1;32m    204\u001b[0m         tx\u001b[39m=\u001b[39;49mtx,\n\u001b[1;32m    205\u001b[0m         proxy\u001b[39m=\u001b[39;49mtx\u001b[39m.\u001b[39;49moutput\u001b[39m.\u001b[39;49mcreate_proxy(\n\u001b[1;32m    206\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mcall_module\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    207\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodule_key,\n\u001b[1;32m    208\u001b[0m             \u001b[39m*\u001b[39;49mproxy_args_kwargs(args, kwargs),\n\u001b[1;32m    209\u001b[0m         ),\n\u001b[1;32m    210\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49moptions,\n\u001b[1;32m    211\u001b[0m     )\n\u001b[1;32m    213\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    214\u001b[0m     \u001b[39m# for lazy modules, run the pre-hooks which will update the type\u001b[39;00m\n\u001b[1;32m    215\u001b[0m     \u001b[39m# TODO mlazos: we don't fully support all of the hooks that exist,\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[39m# so restrict using __call__ only to lazy modules for now\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msource, (\n\u001b[1;32m    218\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMust provide a valid source in order to inline, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    219\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39msince inlined function may have default args which must be guarded.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    220\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_dynamo/variables/builder.py:754\u001b[0m, in \u001b[0;36mwrap_fx_proxy\u001b[0;34m(tx, proxy, example_value, **options)\u001b[0m\n\u001b[1;32m    753\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrap_fx_proxy\u001b[39m(tx, proxy, example_value\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions):\n\u001b[0;32m--> 754\u001b[0m     \u001b[39mreturn\u001b[39;00m wrap_fx_proxy_cls(\n\u001b[1;32m    755\u001b[0m         target_cls\u001b[39m=\u001b[39;49mTensorVariable,\n\u001b[1;32m    756\u001b[0m         tx\u001b[39m=\u001b[39;49mtx,\n\u001b[1;32m    757\u001b[0m         proxy\u001b[39m=\u001b[39;49mproxy,\n\u001b[1;32m    758\u001b[0m         example_value\u001b[39m=\u001b[39;49mexample_value,\n\u001b[1;32m    759\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49moptions,\n\u001b[1;32m    760\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_dynamo/variables/builder.py:789\u001b[0m, in \u001b[0;36mwrap_fx_proxy_cls\u001b[0;34m(target_cls, tx, proxy, example_value, ignore_subclass, **options)\u001b[0m\n\u001b[1;32m    787\u001b[0m \u001b[39mwith\u001b[39;00m preserve_rng_state():\n\u001b[1;32m    788\u001b[0m     \u001b[39mif\u001b[39;00m example_value \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 789\u001b[0m         example_value \u001b[39m=\u001b[39m get_fake_value(proxy\u001b[39m.\u001b[39;49mnode, tx)\n\u001b[1;32m    791\u001b[0m     \u001b[39m# Handle recursive calls here\u001b[39;00m\n\u001b[1;32m    792\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(example_value, FakeTensor):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_dynamo/utils.py:1173\u001b[0m, in \u001b[0;36mget_fake_value\u001b[0;34m(node, tx)\u001b[0m\n\u001b[1;32m   1169\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(\n\u001b[1;32m   1170\u001b[0m     cause, torch\u001b[39m.\u001b[39mfx\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39msymbolic_shapes\u001b[39m.\u001b[39mGuardOnDataDependentSymNode\n\u001b[1;32m   1171\u001b[0m ):\n\u001b[1;32m   1172\u001b[0m     unimplemented(\u001b[39m\"\u001b[39m\u001b[39mguard on data-dependent symbolic int/float\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1173\u001b[0m \u001b[39mraise\u001b[39;00m TorchRuntimeError() \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "\u001b[0;31mTorchRuntimeError\u001b[0m: \n\nfrom user code:\n   File \"/root/.cache/huggingface/modules/transformers_modules/THUDM/chatglm-6b/1d240ba371910e9282298d4592532d7f0f3e9f3e/modeling_chatglm.py\", line 926, in forward\n    inputs_embeds = self.word_embeddings(input_ids)\n\nSet torch._dynamo.config.verbose=True for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    torch._dynamo.config.suppress_errors = True\n"
     ]
    }
   ],
   "source": [
    "class ModifiedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        return model(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            labels=inputs[\"labels\"],\n",
    "        ).loss\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_path,\n",
    "    remove_unused_columns=False,\n",
    "    # evaluation_strategy=IntervalStrategy.STEPS,\n",
    "    # eval_steps=get_steps_in_epoch(0.1),\n",
    "    save_strategy=IntervalStrategy.STEPS,\n",
    "    save_steps=get_steps_in_epoch(0.1),\n",
    "    logging_strategy=IntervalStrategy.STEPS,\n",
    "    logging_steps=get_steps_in_epoch(0.02),\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    # gradient_accumulation_steps=1,\n",
    "    # per_device_eval_batch_size=per_device_train_batch_size,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    max_steps=max_steps,\n",
    "    warmup_ratio=0.1,\n",
    "    # load_best_model_at_end=True,\n",
    "    # metric_for_best_model=\"accuracy\",\n",
    "    torch_compile=True,\n",
    "    dataloader_pin_memory=True,\n",
    "    dataloader_num_workers=8,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "def inner_data_collator(features: list) -> dict:\n",
    "    return data_collator(tokenizer, features)\n",
    "\n",
    "trainer = ModifiedTrainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    args=training_args,\n",
    "    data_collator=inner_data_collator,\n",
    ")\n",
    "\n",
    "resume_from_checkpoint = None\n",
    "# resume_from_checkpoint = path.join(output_path, \"checkpoint-6410\")\n",
    "trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
    "# save model\n",
    "model.save_pretrained(output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
