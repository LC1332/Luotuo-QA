{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from infer import get_model, infer, format_context\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "\n",
    "import copy\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "import torch.utils.checkpoint\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.nn.utils import skip_init\n",
    "from typing import Optional, List, Callable, Dict, Any\n",
    "\n",
    "from transformers.generation.utils import LogitsProcessorList, StoppingCriteriaList, GenerationConfig\n",
    "\n",
    "from transformers.utils import logging\n",
    "logger = logging.get_logger(__name__)\n",
    "\n",
    "model_name: str = \"THUDM/chatglm-6b\"\n",
    "peft_path: str = \"silk-road/luotuo-qa-lora-0.1\"\n",
    "model_revision: str = \"969290547e761b20fdb96b0602b4fd8d863bbb85\"\n",
    "with_origin_model: bool = True\n",
    "\n",
    "model = get_model(model_name, peft_path)\n",
    "origin_model = None\n",
    "if with_origin_model:\n",
    "    origin_model = get_model(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, revision = model_revision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "story = '''\n",
    "长妈妈曾经讲给我一个故事听：先前，有一个读书人住在古庙里用功，晚间， 在院子里纳凉的时候，突然听到有人在叫他。答应着，四面看时，却见一个美女的 脸露在墙头上，向他一笑，隐去了。他很高兴；但竟给那走来夜谈的老和尚识破了 机关。说他脸上有些妖气，一定遇见“美女蛇”了；这是人首蛇身的怪物，能唤人 名，倘一答应，夜间便要来吃这人的肉的。他自然吓得要死，而那老和尚却道无妨 ，给他一个小盒子，说只要放在枕边，便可高枕而卧。他虽然照样办，却总是睡不 着，——当然睡不着的。到半夜，果然来了，沙沙沙！门外象是风雨声。他正抖作 一团时，却听得豁的一声，一道金光从枕边飞出，外面便什么声音也没有了，那金 光也就飞回来，敛在盒子里。后来呢？后来，老和尚说，这是飞蜈蚣，它能吸蛇的 脑髓，美女蛇就被它治死了。\n",
    "'''\n",
    "question = '是谁识破了机关？'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def continue_generate(\n",
    "        model,\n",
    "        input_ids: torch.Tensor,\n",
    "        append_ids: torch.Tensor,\n",
    "        generation_config: Optional[GenerationConfig] = None,\n",
    "        logits_processor: Optional[LogitsProcessorList] = None,\n",
    "        stopping_criteria: Optional[StoppingCriteriaList] = None,\n",
    "        prefix_allowed_tokens_fn: Optional[Callable[[int, torch.Tensor], List[int]]] = None,\n",
    "        **kwargs,\n",
    "):\n",
    "    batch_size, input_ids_seq_length = input_ids.shape[0], input_ids.shape[-1]\n",
    "\n",
    "    if generation_config is None:\n",
    "        generation_config = model.generation_config\n",
    "    generation_config = copy.deepcopy(generation_config)\n",
    "    model_kwargs = generation_config.update(**kwargs)\n",
    "    bos_token_id, eos_token_id = generation_config.bos_token_id, generation_config.eos_token_id\n",
    "\n",
    "    if isinstance(eos_token_id, int):\n",
    "        eos_token_id = [eos_token_id]\n",
    "\n",
    "    has_default_max_length = kwargs.get(\"max_length\") is None and generation_config.max_length is not None\n",
    "    if has_default_max_length and generation_config.max_new_tokens is None:\n",
    "        warnings.warn(\n",
    "            f\"Using `max_length`'s default ({generation_config.max_length}) to control the generation length. \"\n",
    "            \"This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we\"\n",
    "            \" recommend using `max_new_tokens` to control the maximum length of the generation.\",\n",
    "            UserWarning,\n",
    "        )\n",
    "    elif generation_config.max_new_tokens is not None:\n",
    "        generation_config.max_length = generation_config.max_new_tokens + input_ids_seq_length\n",
    "        if not has_default_max_length:\n",
    "            logger.warn(\n",
    "                f\"Both `max_new_tokens` (={generation_config.max_new_tokens}) and `max_length`(=\"\n",
    "                f\"{generation_config.max_length}) seem to have been set. `max_new_tokens` will take precedence. \"\n",
    "                \"Please refer to the documentation for more information. \"\n",
    "                \"(https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\",\n",
    "                UserWarning,\n",
    "            )\n",
    "\n",
    "    if input_ids_seq_length >= generation_config.max_length:\n",
    "        input_ids_string = \"decoder_input_ids\" if model.config.is_encoder_decoder else \"input_ids\"\n",
    "        logger.warning(\n",
    "            f\"Input length of {input_ids_string} is {input_ids_seq_length}, but `max_length` is set to\"\n",
    "            f\" {generation_config.max_length}. This can lead to unexpected behavior. You should consider\"\n",
    "            \" increasing `max_new_tokens`.\"\n",
    "        )\n",
    "\n",
    "    # 2. Set generation parameters if not already defined\n",
    "    logits_processor = logits_processor if logits_processor is not None else LogitsProcessorList()\n",
    "    stopping_criteria = stopping_criteria if stopping_criteria is not None else StoppingCriteriaList()\n",
    "\n",
    "    logits_processor = model._get_logits_processor(\n",
    "        generation_config=generation_config,\n",
    "        input_ids_seq_length=input_ids_seq_length,\n",
    "        encoder_input_ids=input_ids,\n",
    "        prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,\n",
    "        logits_processor=logits_processor,\n",
    "    )\n",
    "\n",
    "    stopping_criteria = model._get_stopping_criteria(\n",
    "        generation_config=generation_config, stopping_criteria=stopping_criteria\n",
    "    )\n",
    "    logits_warper = model._get_logits_warper(generation_config)\n",
    "    \n",
    "    input_ids = torch.cat([input_ids, append_ids], dim=-1)\n",
    "    unfinished_sequences = input_ids.new(input_ids.shape[0]).fill_(1)\n",
    "    scores = None\n",
    "    while True:\n",
    "        model_inputs = model.prepare_inputs_for_generation(input_ids, **model_kwargs)\n",
    "        # forward pass to get next token\n",
    "        outputs = model(\n",
    "            **model_inputs,\n",
    "            return_dict=True,\n",
    "            output_attentions=False,\n",
    "            output_hidden_states=False,\n",
    "        )\n",
    "\n",
    "        next_token_logits = outputs.logits[:, -1, :]\n",
    "\n",
    "        # pre-process distribution\n",
    "        next_token_scores = logits_processor(input_ids, next_token_logits)\n",
    "        next_token_scores = logits_warper(input_ids, next_token_scores)\n",
    "\n",
    "        # sample\n",
    "        probs = nn.functional.softmax(next_token_scores, dim=-1)\n",
    "        if generation_config.do_sample:\n",
    "            next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)\n",
    "        else:\n",
    "            next_tokens = torch.argmax(probs, dim=-1)\n",
    "\n",
    "        # update generated ids, model inputs, and length for next step\n",
    "        input_ids = torch.cat([input_ids, next_tokens[:, None]], dim=-1)\n",
    "        model_kwargs = model._update_model_kwargs_for_generation(\n",
    "            outputs, model_kwargs, is_encoder_decoder=model.config.is_encoder_decoder\n",
    "        )\n",
    "        unfinished_sequences = unfinished_sequences.mul((sum(next_tokens != i for i in eos_token_id)).long())\n",
    "\n",
    "        # stop when each sentence is finished, or if we exceed the maximum length\n",
    "        if unfinished_sequences.max() == 0 or stopping_criteria(input_ids, scores):\n",
    "            break\n",
    "    return input_ids\n",
    "\n",
    "def question_answer_infer(model, tokenizer: PreTrainedTokenizer, story, question, max_length=2048):\n",
    "    append_text = f\"\"\"问题转义为:{question}\n",
    "答案为:\"\"\"\n",
    "\n",
    "    input_token_ids = tokenizer.encode(format_context(story, question))\n",
    "    input_ids = torch.LongTensor([input_token_ids]).to(model.device)\n",
    "    append_token_ids = tokenizer.encode(append_text)\n",
    "    append_ids = torch.LongTensor([append_token_ids]).to(model.device)\n",
    "    out = continue_generate(model, input_ids, append_ids, \n",
    "        max_length = max_length, \n",
    "        do_sample=True, \n",
    "        top_p=0.2, \n",
    "        temperature=0.95, \n",
    "        logits_processor=None,\n",
    "    )[0]\n",
    "    out_text = tokenizer.decode(list[int](out)[len(input_token_ids) + len(append_token_ids):])\n",
    "    answer = out_text.replace(\"\\nEND\", \"\").strip()\n",
    "    print(f\"question_answer_infer: ###{answer}###\")\n",
    "\n",
    "    from infer import gen\n",
    "    gen_out = gen(model, tokenizer, format_context(story, question) + append_text)\n",
    "    print(f\"default gen: ###{gen_out}###\")\n",
    "    \n",
    "    return answer\n",
    "\n",
    "for i in range(10):\n",
    "    question_answer_infer(model, tokenizer, story, question)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
